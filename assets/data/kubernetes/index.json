{"hash":"02799b688b4cb7ace869e49ff69fbf95eb6f42e6","data":{"post":{"title":"Kubernetes","content":"\n## Kubernetes\nKubernetes - открытая платформа созданная для автомтаического деплоя, масштабирования и оперирования контейнерами приложений.\n\nПортативная расширяемая платформа с открытым исходным кодом для управления контейнеризованными рабочими нагрузками и сервисами которая облегчает как декларативную настройку так и автоматизацию.\n\nKubernetes состоит из нод. Как правило рекомендуют использовать не менее 3х нод для Kubernetes. \nMaster Node и 2 Work Nodes. \n\n**Master Node** - отвечает за поддержание желаемого состояния для вашего кластера.\nС Master Node взаимодействует kubectl - интерфейс командной строки, который позволяет через командную строку управлять Kubernetes кластером.\n\nС другой стороны у нас есть пользователь, который через интернет обращается к нашему приложению через Work Node и через kube-proxy ходит непосредственно на pod'ы.Основные фундаментальные концепции Kubernetes -  это pod и node, а также kublet, kube-proxy, etcd.\n\n**Nodes(узлы)** - это виртуальные либо физические машины в Kubernetes кластере на которых будут запускаться контейнеры.\nНода содержит kublet, Docker, kube-proxy. Также нода может содержать 1 или несколько под.\n\n**Pod** - минимальный юнит в Kubernetes с которым можно взаимодействовать, абстрактный объект Kubernetes представляющий группу из одного или нескольких контейнеров приложения (например Docker).\n\nПоды можно создавать, деплоить и удалять. Одна пода - один процесс в кластере. \nПод содержит: Docker container, storage resources, уникальный IP. \n\n**Cluster** - совокупность мастер-сервисов и нод.\n\n**Namespace** - это способ разделения ресурсов кластера между несколькими пользователями.\nНапример, namespace команд, проектов и тд.\n\nЧтобы зайти в кластер и начать запускать команды нужно установить kubectl.\n\nkubectl - это инструмент командной строки kubernetes, который позволяет запускать команды для кластеров Kubernetes. Вы можете использовать kubectl для развертывания приложений проверки и управлени ресурсов кластера а также просмотра логов\n\n### Жизненный цикл Pod\n\n- Pending - ожидание, под ждет ресурсов. Под был принят кластером но один или несколько контейнеров еще не были запущены и нужно подождать.\n- Running - запуск, созданы контейнеры необходимые для пода и запуска непосредственно на этой ноде. Под привязан к узлу и все контейнеры созданы.\n- ContainerCreating - собираются контейнеры.\n- Succeeded/Completed - успешный запуск, все контейнеры созданы, работают, нода запущена.\n- CreateContainerConfigError - ошибка конфигурации.\n- Failed - неуспешный запуск, запуск зафейлился приходит response != 1\n- CrashLoopBackOff - под уходит в бесконечный цикл. Под был запущен крашнулся перезапустился и заново крашнулся (установлено значение restartPolicy: Always) нужно изучить логи.\n- Terminating -  трафик не идет на под, под тушится после его удаления\n\nЧтобы работало автодоплнение нужно выполнить команду:\n\n```\nsource <(kubectl completion bash)\n```\n\nДалее нужно получить доступ к кластеру:\n\n- Получить список всех неймспейсов\n```\nkubectl get namespaces\n```\n\n- получить список всех подов во всех неймспейсах\n```\nkubectl get pods --all-namespaces\n```\n\n- получить список всех подов во всех неймспейсах там где есть название ssr с подробным выводом логов\n```\nkubectl get pods --all-namespaces -o wide |grep ssr\n```\n\n- получить список подов в определенном неймспейсе\n```\nkubectl get pods -n core-team\n```\n\n```\nkubectl get pods -n core-team|grep besida-madmax\n```\n\nПосле того как вышел список подов мы можем увидеть статусы в которых они находятся:\n\n- Обзор запущенного пода\n```\nkubectl -n core-team describe pod besida-trunk-ua-685d5d4f-hpdw2\n```\n\n- Вывести логи пода \n```\nkubectl -n core-team logs besida-trunk-ua-685d5d4f-hpdw2\n```\n\nотображает логи на лету\n```\nkubectl -n core-team logs -f besida-trunk-ua-685d5d4f-hpdw2\n```\n\n### Controllers\n\nУправляется Controller Manager'ом. \n\nВиды контроллеров:\n- ReplicaSets - проверяет что необходимое количество pod запущено все время. Если pods стало меньше (например одна изпод закрешилась), то replicaSet создаст новую. ReplicaSets существует не самостоятельно а в рамках Deployment.\n\n- Deployments - предоставляет декларативное описание для апдейта ReplicaSet и Pod. В ранних версиях Kubernetes вместо ReplicaSet и Deployment использовался Replication Controller. Но это нарушало принцип single responsibilities и в дальнейшем он был разделен и облегчилась задача rollback'a - если во время деплоймента что то пошло не так то в текущих условиях легко откатиться назад и востановить работоспособность приложения.  \n\n- DeamonSets - проверяет что на каждой ноде запущен экземпляр конкретной поды. Если ноды добавляются в кластер или удаляются из кластера то DeamonSet добавить или удалить поды на этой ноде. Удаление DeamonSet означает удаление всех под из кластера.\n\n- Jobs - это процесс верхнего уровня для пода. Используется когда нужно запустить pod для выполнения какой то задачи один раз или по расписанию. Типичный пример - cron job.\n\n- Services - позволяет сетевое взаимодействие между деплойментами. Необходимы, когда нужно, чтобы поды из разных деплойментов взаимодействовали между собой. \n\nНапример: FrontEnd Pod взаимодействует с BackEnd Pods через Backend Service.\n\n### Виды сервисов:\nInternal - ip-адрес доступен только внутри кластера. \n\nExternal - эндпоинт доступен по ip адресу ноды(такой сервис называют NodePod). \n\nLoad Balancer - открывает приложение в интернет через лоад балансер (обычно используется когда кубернетис кластер развернут в облаке (GCP, AWS, Azure).\n\n\n### Labels\n\nПара ключ/значение, может быть присоединена к таким объектам как поды, сервисы и доплойменты. Используются пользователями Кубернетис для идентификации аттрибутов для объектов. Уникальны в пределах объекта. \n\nПример: \"environment\": \"dev\", \"environment\": \"qa\", \"environment\": \"prod\"\n\nLabels как правило используются не одни а с selectors.\n\n### Selectors\n\n- equality-based: '=' и '!='\n- Set-based: 'IN', 'NOTIN' и 'EXISTS'\n\nLabels и Selectors обычно используются в kubectl командах для получения списков объектов и их фильтрации. НапримерЖ получение списка под на QA env.\n\n### Namespaces\n\nКонцепция неймспейсов позволяет реализовать множество виртуальных кластеров внутри одного физического кластера. Это полезно, когда есть необходимость разделять ресурсы физического кластера между командами и контролировать доступ к ресурсам.\n\n### Kublet\n\n- Запущен на каждой work-ноде\n- Коммуницирует с API сервером, который запущен на Master Node\n- Запускает контейнеры для под через docker engine\n- Подключает и запускает диски и сикреты для под\n- Запускает хелсчеки для проверки статусов под/нод и сообщает статус API серверу\n\n### Kube-proxy\n\n- Запущен на каждой work-ноде\n- Рефлицирует сетевой трафик для сервисов (NodePort и LoadBalancer)\nконфигурирует правила сети на узлах. При помощи них разрешаются сетевые подключения к вашими подам изнутри и снаружи кластера.\n\n\n### Режимы Kube-proxy\n\n- User space mode (наиболее широко используемый)\n- Iptables mode\n- Ipvs mode (alpha version)\n\n### etcd\nРаспределённое и высоконадёжное хранилище данных в формате \"ключ-значение\", которое используется как основное хранилище всех данных кластера в Kubernetes.\n\n### kube-scheduler\nКомпонент плоскости управления, который отслеживает созданные поды без привязанного узла и выбирает узел, на котором они должны работать.\n\n\n## Команды Kubernetes\n\n- получить список подов\n```\nkubectl get pods\n```\n\n- получить список сервисов\n```\nkubectl get services\n```\n\n- получить список деплойментов\n```\nkubectl get deployments\n```\n\n- поднимаем selenium-hub\n```\nkubectl create -f selenium-hub-deployment.yaml\n```\n\n- поднимаем selenium-hub-svc\n```\nkubectl create -f selenium-hub-svc.yaml\n```\n\n- поднимаем selenium-node\n```\nkubectl create -f selenium-node-chrome-deployment.yaml\n```\n\n\n\n\n## CI/CD microservices\n\nCI - Continious Integration это когда разработчики интегрируют свои код в общий репозиторий на постоянной основе и постоянно проходят некий quality gate который показывает что их код синтегрировался корректно. \n\nCD - Continious Delivery это когда артефакт который мы собрали в рамках Continious Integration и начинаем поставлять его на разные окружения.\nContinious De[loyment это когда мы в процессе Continious Delivery не ждем ручного апрува а автоматом через энвайронменты проводим и выкатываем на продакшен.\n\nКак этого достичь:\n1. Докеризация микросервисов\nнужно получить артефакт который будет неизменным - нужно быть уверенным что тот артефакт который мы собрали в таком же виде дойдет до продакшена\n2. Мы можем присваивать артефакту теги и тем самым продвигать его на следущую стадию\n3. Мы не завязываемся на технический стек\n4. Это прощает деплоймент, управление окружением, конфигурацию и т.д.\n5. Эфективное использование ресурсов\n\nУ нас есть контейнер - запущенный процесс который представляет наш микросервис и image - immutable артефакт который мы собрали в рамках  CI и выложили его в registry - реестр images, где хранятся наши артефакты.\n\n## Версионирование\n\nСемантическое версионирование - \nглавная версия отвечает за то какие знаковые изменения были сделаны в микросервисе(то что мы не поломали API), если мы не имеем обратной совместимости то увеличиваем эту версию либо по большим релизом и с каждым релизом увеличиваем;\nминорная версия - либо начинаем каждый раз с нуля в рамках каждого нового большошо релиза либо если девелопим интерациями то используют номер интерации, тогда можно быстро востановить когда эта версия была выпущена.\nпатч версия - для хот фиксов \nкомит хештег как суффикс и дата как дополнительный суффикс\n\n## CI pipeline\n\nРазработчик делает комит в гит репозиторий на CI делается: build code, run unit tests, build image, push image(пушится в реестр контейнеров). На выходе мы получаем кодовые артифакты, результаты тестов, image контейнер.\n\nQuality Gates:\n- unit tests\n- integration tests\n- static code analysis\n- api tests\n- contract tests\n- security checks\n\nЧтобы поставить полностью всю систему нужно знать версии всех микросервисов.\n\nЗдесь нужно учитывать совместимость - это когда мы взяли набор микросервисов подняли провели тесты и после этого мы говорим что этот набор сервисов совместим \n\nПоэтому нужно сохранять нобор версий этих сервисов как отдельный артефакт \nЭтот артефакт можно положить в систему контроля версий(например гит)\nДальше в Continious Delivery будет участвовать этот набор. и дальше можно промоутить этот артефакт между разными окружениями. Если мы добавим зависимость на какую то версию postgress или elastic то мы получим полную совместимсоть\n\nЭто делается с помощью property файла и прописываем версии.\nМожно использовать helm.\n\n**Пример:**\n\nУ нас есть 3 сервиса с соответсвущими версиями и мы их собрали в compatible set. \n\nИ тут появляется доработка в одном из сервисов и появляется его новая версия и это новый кандидат мы хотим его продвинуть. \n\nМы собираем новый set с этим кандидатом и пытаемся его запустить и билд падает(возможно оказалась проблема в несовместимости например с ui частью).\n\nИ появилась необходимость сделать исправление в другом микросервисе, разработчики делают исправление, получают новую версию и хотят ее задеплоить. И тут вступает фактор, что нужно подхватывать всех новых кандидатов, потому что если бы подхватили только последний кандидат, он могбы упасть так как ожидает исправлений, которые появились в первом сервисе.\n\nЕсли билд проходит успешно, то мы делаем совместимый между собой set и сохраняем его в сисетму контроля версий.\n\nЕсли билд падает, то артифакт невидим и нет возможность такой билд куда-то задеплоить просто так, не пройдя тест на совместимость. \n\nПосле того как артефакт появился в системе контроля версий, то либо jenkins джоба это проверяет, либо оператор в Kubernetes переодически мониторит что появляется и говорит что надо это деплоить - вручную или по расписанию. \n\nFlux CD мониторит появление новых images в гите и говорит что можно их задеплоить.\n\n## Выполняем деплой на окружение\n\nДля этого мы создаем Jenkins джобу или github actions которая показывает нам список окружений который нам доступен: dev, qa, stage, prod либо оставляем создаем новое окружение. Также показываются для выбора не отдельные микросервисы а показыватся сеты. Еще нужно указать TTL(Time To Live) окружение будет автоматически очищаться по истечению времени. \n\nНо если будет много окружений - это займет все ресурс и для решения этой проблемы нужен Kubernetes.\n\nКак ускорить создание окружения в Kubernetes:\n- каждое окружение уходит в отдельый namespace \n- конфигурируем минимальные лимиты и scale фактор \n- выносим основные внешние ресурсы (DB, Elastcsearch, Redis, Kafka)\n- используем готовые images с данными\n- сконфигурируем все тулы \n- асинхронное удаление окружение\n- задаем TTL\n\n\nzhuk.__\n\n","description":"Kubernetes","image":"null","category":{"id":"testops","title":"testops","path":"/category/testops/"},"date":"27/07/2022","path":"/kubernetes/","links":[{"title":"","items":[{"title":"Микросервисная архитектура","link":"//mikroservisnaya-arhitektura//"},{"title":"Docker","link":"/docker/"},{"title":"Kubernetes","link":"/kubernetes/"},{"title":"Linux","link":"/linux/"},{"title":"Общие вопросы TestOps","link":"/obshhie-voprosy-test-ops/"}]}],"headings":[{"value":"Kubernetes","anchor":"#kubernetes"},{"value":"Команды Kubernetes","anchor":"#команды-kubernetes"},{"value":"CI/CD microservices","anchor":"#cicd-microservices"},{"value":"Версионирование","anchor":"#версионирование"},{"value":"CI pipeline","anchor":"#ci-pipeline"},{"value":"Выполняем деплой на окружение","anchor":"#выполняем-деплой-на-окружение"}],"subtitles":[{"depth":2,"value":"Kubernetes","anchor":"#kubernetes"},{"depth":3,"value":"Жизненный цикл Pod","anchor":"#жизненный-цикл-pod"},{"depth":3,"value":"Controllers","anchor":"#controllers"},{"depth":3,"value":"Виды сервисов:","anchor":"#виды-сервисов"},{"depth":3,"value":"Labels","anchor":"#labels"},{"depth":3,"value":"Selectors","anchor":"#selectors"},{"depth":3,"value":"Namespaces","anchor":"#namespaces"},{"depth":3,"value":"Kublet","anchor":"#kublet"},{"depth":3,"value":"Kube-proxy","anchor":"#kube-proxy"},{"depth":3,"value":"Режимы Kube-proxy","anchor":"#режимы-kube-proxy"},{"depth":3,"value":"etcd","anchor":"#etcd"},{"depth":3,"value":"kube-scheduler","anchor":"#kube-scheduler"},{"depth":2,"value":"Команды Kubernetes","anchor":"#команды-kubernetes"},{"depth":2,"value":"CI/CD microservices","anchor":"#cicd-microservices"},{"depth":2,"value":"Версионирование","anchor":"#версионирование"},{"depth":2,"value":"CI pipeline","anchor":"#ci-pipeline"},{"depth":2,"value":"Выполняем деплой на окружение","anchor":"#выполняем-деплой-на-окружение"}],"tags":[{"id":"Автоматизация тестирования","title":"Автоматизация тестирования","path":"/tag/%D0%90%D0%B2%D1%82%D0%BE%D0%BC%D0%B0%D1%82%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F%20%D1%82%D0%B5%D1%81%D1%82%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F/"}]},"comments":{"edges":[]}},"context":{}}